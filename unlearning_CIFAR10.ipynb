{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzeKVh3pD-zf",
        "outputId": "3d7a5910-1b04-4a11-d7d3-f92231fbf50b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.46.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from shap) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from shap) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from shap) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (24.2)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from shap) (0.61.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->shap) (0.44.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V6YXq1oMM3bj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import linear_model, model_selection\n",
        "import shap\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.models import resnet18\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.prune as prune\n",
        "from copy import deepcopy\n",
        "from math import sqrt\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Running on device:\", DEVICE.upper())\n",
        "\n",
        "# manual random seed is used for dataset partitioning\n",
        "# to ensure reproducible results across runs\n",
        "RNG = torch.Generator().manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF7zXiGLM3bk"
      },
      "source": [
        "# Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "g5G7DBCoM3bl",
        "outputId": "0bfdaf2a-b9fa-48f6-e113-4cb227392464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7720c4c16182>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mclass_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mclass_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# download and pre-process CIFAR10\n",
        "normalize = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=normalize\n",
        ")\n",
        "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "class_indices = [[] for _ in range(10)]\n",
        "\n",
        "for idx, (_, label) in enumerate(train_set):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "print(class_indices)\n",
        "\n",
        "# we split held out data into test and validation set\n",
        "held_out = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=normalize\n",
        ")\n",
        "test_set, val_set = torch.utils.data.random_split(held_out, [0.5, 0.5], generator=RNG)\n",
        "print(test_set)\n",
        "test_loader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "val_loader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "test_class_indices = [[] for _ in range(10)]\n",
        "\n",
        "for idx, (_, label) in enumerate(test_set):\n",
        "    test_class_indices[label].append(idx)\n",
        "\n",
        "print(test_class_indices)\n",
        "\n",
        "# Znalezienie indeksów dla klasy 0 i pozostałych klas\n",
        "indices_class_0 = test_class_indices[0]  # Indeksy próbek klasy 0\n",
        "indices_other_classes = [idx for i in range(1, 10) for idx in test_class_indices[i]]  # Indeksy pozostałych klas\n",
        "\n",
        "# Tworzenie podzbiorów danych\n",
        "test_set_0 = Subset(test_set, indices_class_0)\n",
        "test_set_no_0 = Subset(test_set, indices_other_classes)\n",
        "\n",
        "# Tworzenie DataLoaderów\n",
        "test_loader_0 = DataLoader(test_set_0, batch_size=128, shuffle=False, num_workers=2)\n",
        "test_loader_no_0 = DataLoader(test_set_no_0, batch_size=128, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSjSOp5wLNO0"
      },
      "outputs": [],
      "source": [
        "print(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9OMjLg2zlq4"
      },
      "source": [
        "# Starting model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Rn6UKX_M3bl"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Pobieramy predefiniowany model ResNet18\n",
        "model = resnet18(weights=None)  # Można użyć `weights='IMAGENET1K_V1'`, ale lepiej trenować od zera\n",
        "model.fc = nn.Linear(512, 10)  # Dostosowanie warstwy końcowej do 10 klas CIFAR-10\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lSCACewz4tG"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ib3CABKz6tb"
      },
      "outputs": [],
      "source": [
        "num_epochs = 40  # Liczba epok\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    model.train()  # Przełącz na tryb trenowania\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Zerowanie gradientów\n",
        "        outputs = model(images)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Obliczenie straty\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Aktualizacja wag\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Trening zakończony!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bui7WbQhw2-d"
      },
      "outputs": [],
      "source": [
        "def full_retraining(train_loader):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  # Pobieramy predefiniowany model ResNet18\n",
        "  model = resnet18(weights=None)  # Można użyć `weights='IMAGENET1K_V1'`, ale lepiej trenować od zera\n",
        "  model.fc = nn.Linear(512, 10)  # Dostosowanie warstwy końcowej do 10 klas CIFAR-10\n",
        "  model = model.to(device)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  num_epochs = 40  # Liczba epok\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      running_loss = 0.0\n",
        "      model.train()  # Przełącz na tryb trenowania\n",
        "\n",
        "      for images, labels in train_loader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()  # Zerowanie gradientów\n",
        "          outputs = model(images)  # Forward pass\n",
        "          loss = criterion(outputs, labels)  # Obliczenie straty\n",
        "          loss.backward()  # Backpropagation\n",
        "          optimizer.step()  # Aktualizacja wag\n",
        "\n",
        "          running_loss += loss.item()\n",
        "\n",
        "      print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "  print(\"Trening zakończony!\")\n",
        "  model.eval()\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9-FVEiHz9ui"
      },
      "outputs": [],
      "source": [
        "model.eval()  # Przełączamy model na tryb ewaluacji\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy na zbiorze testowym: {accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uc32Ytc2xTH4"
      },
      "outputs": [],
      "source": [
        "def data_to_forget(train, class_indices, label, size):\n",
        "  sampled_class = []\n",
        "  if label == -1:\n",
        "    for i in range(10):\n",
        "      print(\"*\")\n",
        "      tmp = random.sample(class_indices[i], int(size/10))\n",
        "      class_indices[i] = list(set(class_indices[i]) - set(tmp))\n",
        "      sampled_class.extend(tmp)\n",
        "  else:\n",
        "    sampled_class = random.sample(class_indices[label], size)  # Wylosowanie 50 próbek\n",
        "    class_indices[label] = list(set(class_indices[label]) - set(sampled_class))\n",
        "\n",
        "  flat_class_indices = [idx for sublist in class_indices for idx in sublist]\n",
        "  forget_set = torch.utils.data.Subset(train_set, sampled_class)\n",
        "  retain_set = torch.utils.data.Subset(train_set, flat_class_indices)\n",
        "\n",
        "  forget_loader = torch.utils.data.DataLoader(\n",
        "      forget_set, batch_size=128, shuffle=True, num_workers=2\n",
        "  )\n",
        "  retain_loader = torch.utils.data.DataLoader(\n",
        "      retain_set, batch_size=128, shuffle=True, num_workers=2, generator=RNG\n",
        "  )\n",
        "  return forget_loader, retain_loader, class_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXEWkDTTn4iO"
      },
      "source": [
        "#  Fauchan - 1st place\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12BWO9ORUoyX"
      },
      "outputs": [],
      "source": [
        "def evaluation(net, dataloader, criterion, device = 'cuda'): ##evaluation function\n",
        "    net.eval()\n",
        "    total_samp = 0\n",
        "    total_acc = 0\n",
        "    total_loss = 0.0\n",
        "    for sample in dataloader:\n",
        "        images, labels = sample['image'].to(device), sample['age_group'].to(device)\n",
        "        _pred = net(images)\n",
        "        total_samp+=len(labels)\n",
        "        #print(f'total_samp={total_samp}')\n",
        "        loss = criterion(_pred, labels)\n",
        "        total_loss += loss.item()\n",
        "        total_acc+=(_pred.max(1)[1] == labels).float().sum().item()\n",
        "        #print(f'total_acc={total_acc}')\n",
        "    #print(f'total_sample={total_samp}')\n",
        "    mean_loss = total_loss / len(dataloader)\n",
        "    mean_acc = total_acc/total_samp\n",
        "    print(f'loss={mean_loss}')\n",
        "    print(f'acc={mean_acc}')\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvLkBLisM3bm"
      },
      "outputs": [],
      "source": [
        "USE_MOCK: bool = False\n",
        "\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR,CosineAnnealingWarmRestarts,StepLR\n",
        "def kl_loss_sym(x,y):\n",
        "    kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
        "    return kl_loss(nn.LogSoftmax(dim=-1)(x),y)\n",
        "def fauchan(\n",
        "        net,\n",
        "        retain_loader,\n",
        "        forget_loader,\n",
        "        val_loader,\n",
        "):\n",
        "    \"\"\"Simple unlearning by finetuning.\"\"\"\n",
        "    print('-----------------------------------')\n",
        "    epochs = 8\n",
        "    retain_bs = 256\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.005,\n",
        "                          momentum=0.9, weight_decay=0)\n",
        "    optimizer_retain = optim.SGD(net.parameters(), lr=0.001*retain_bs/64, momentum=0.9, weight_decay=1e-2)\n",
        "    ##the learning rate is associated with the batchsize we used\n",
        "    optimizer_forget = optim.SGD(net.parameters(), lr=3e-4, momentum=0.9, weight_decay=0)\n",
        "    total_step = int(len(forget_loader)*epochs)\n",
        "    retain_ld = DataLoader(retain_loader.dataset, batch_size=retain_bs, shuffle=True)\n",
        "    retain_ld4fgt = DataLoader(retain_loader.dataset, batch_size=256, shuffle=True)\n",
        "    scheduler = CosineAnnealingLR(optimizer_forget, T_max=total_step, eta_min=1e-6)\n",
        "    \"\"\"if USE_MOCK: ##Use some Local Metric as reference\n",
        "        net.eval()\n",
        "        print('Forget')\n",
        "        evaluation(net, forget_loader, criterion)\n",
        "        print('Valid')\n",
        "        evaluation(net, validation_loader, criterion)\"\"\"\n",
        "    net.train()\n",
        "    for inputs, targets in forget_loader: ##First Stage\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        uniform_label = torch.ones_like(outputs).to(DEVICE) / outputs.shape[1] ##uniform pseudo label\n",
        "        loss = kl_loss_sym(outputs, uniform_label) ##optimize the distance between logits and pseudo labels\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \"\"\"if USE_MOCK:\n",
        "        print('Forget')\n",
        "        evaluation(net,forget_loader,criterion)\n",
        "        print('Valid')\n",
        "        evaluation(net, validation_loader,criterion)\n",
        "        print(f'epoch={epochs} and retain batch_sz={retain_bs}')\"\"\"\n",
        "    net.train()\n",
        "    for ep in range(epochs): ##Second Stage\n",
        "        net.train()\n",
        "        for (inputs_forget, outputs_forget), (inputs_retain, outputs_retain) in zip(forget_loader, retain_ld4fgt):##Forget Round\n",
        "            t = 1.15 ##temperature coefficient\n",
        "            inputs_forget, inputs_retain = inputs_forget.to(DEVICE), inputs_retain.to(DEVICE)\n",
        "            optimizer_forget.zero_grad()\n",
        "            outputs_forget,outputs_retain = net(inputs_forget),net(inputs_retain).detach()\n",
        "            loss = (-1 * nn.LogSoftmax(dim=-1)(outputs_forget @ outputs_retain.T/t)).mean() ##Contrastive Learning loss\n",
        "            loss.backward()\n",
        "            optimizer_forget.step()\n",
        "            scheduler.step()\n",
        "        for inputs, labels in retain_ld: ##Retain Round\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer_retain.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_retain.step()\n",
        "        \"\"\"if USE_MOCK:\n",
        "            print(f'epoch {ep}:')\n",
        "            print('Retain')\n",
        "            evaluation(net, retain_ld, criterion)\n",
        "            print('Forget')\n",
        "            evaluation(net, forget_loader, criterion)\n",
        "            print('Valid')\n",
        "            evaluation(net, validation_loader, criterion)\"\"\"\n",
        "    print('-----------------------------------')\n",
        "    return net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsOIZhKChv2G"
      },
      "source": [
        "# Kookmin - 2 place"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE4rsGYSh6eJ"
      },
      "outputs": [],
      "source": [
        "sch = 'linear'\n",
        "init_rate = 0.3\n",
        "init_method = 'snip_little_grad'\n",
        "lr = 0.001\n",
        "epoch = 5\n",
        "weight_decay = 5e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4IYp9pxiKli"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "\n",
        "class LinearAnnealingLR(_LRScheduler):\n",
        "    def __init__(self, optimizer, num_annealing_steps, num_total_steps):\n",
        "        self.num_annealing_steps = num_annealing_steps\n",
        "        self.num_total_steps = num_total_steps\n",
        "\n",
        "        super().__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self._step_count <= self.num_annealing_steps:\n",
        "            return [base_lr * self._step_count / self.num_annealing_steps for base_lr in self.base_lrs]\n",
        "        else:\n",
        "            return [base_lr * (self.num_total_steps - self._step_count) / (self.num_total_steps - self.num_annealing_steps) for base_lr in self.base_lrs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEK8_pTyi5L-"
      },
      "outputs": [],
      "source": [
        "class Masker(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, mask):\n",
        "        ctx.save_for_backward(mask)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad):\n",
        "        mask, = ctx.saved_tensors\n",
        "        return grad * mask, None\n",
        "\n",
        "\n",
        "class MaskConv2d(nn.Conv2d):\n",
        "    def __init__(self, mask, in_channels, out_channels, kernel_size, stride=1,\n",
        "                 padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device='cpu'):\n",
        "        super(MaskConv2d, self).__init__(in_channels, out_channels, kernel_size, stride,\n",
        "                                     padding, dilation, groups, bias, padding_mode, device=device)\n",
        "        self.mask = mask\n",
        "\n",
        "    def forward(self, input):\n",
        "        masked_weight = Masker.apply(self.weight, self.mask)\n",
        "        return super(MaskConv2d, self)._conv_forward(input, masked_weight, self.bias)\n",
        "\n",
        "@torch.no_grad()\n",
        "def re_init_model_random_zero_grad_Maskconv(model, px):\n",
        "    print(\"Apply Unstructured random re init no grads Globally (all conv layers)\")\n",
        "    for name, m in list(model.named_modules()):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            mask = torch.zeros_like(m.weight, device=DEVICE).bool()\n",
        "            nparams_toprune = round(px*mask.nelement())\n",
        "\n",
        "            prob = torch.rand_like(mask.float(), device=DEVICE)\n",
        "            topk = torch.topk(prob.view(-1), k=nparams_toprune)\n",
        "            mask.view(-1)[topk.indices] = True\n",
        "\n",
        "            new_conv = MaskConv2d(mask, m.in_channels, m.out_channels, m.kernel_size, m.stride,\n",
        "                 m.padding, m.dilation, m.groups, m.bias!=None, m.padding_mode, device=DEVICE)\n",
        "            #nn.init.kaiming_normal_(new_conv.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            new_conv.weight.data[~mask] = m.weight[~mask]\n",
        "            setattr(model, name, new_conv)\n",
        "\n",
        "@torch.no_grad()\n",
        "def re_init_model_random_little_grad_Maskconv_fanout(model, px):\n",
        "    print(\"Apply Unstructured re_init_model_random_little_grad_Maskconv_fanout (all conv layers)\")\n",
        "    for name, m in list(model.named_modules()):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            mask = torch.zeros_like(m.weight, device=DEVICE).bool()\n",
        "            nparams_toprune = round(px*mask.nelement())\n",
        "\n",
        "            prob = torch.rand_like(mask.float(), device=DEVICE)\n",
        "            topk = torch.topk(prob.view(-1), k=nparams_toprune)\n",
        "            mask.view(-1)[topk.indices] = True\n",
        "            grad_mask = mask.clone().float()\n",
        "            grad_mask[grad_mask==0] += 0.1\n",
        "\n",
        "            new_conv = MaskConv2d(grad_mask, m.in_channels, m.out_channels, m.kernel_size, m.stride,\n",
        "                 m.padding, m.dilation, m.groups, m.bias!=None, m.padding_mode, device=DEVICE)\n",
        "            nn.init.kaiming_normal_(new_conv.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            new_conv.weight.data[~mask] = m.weight[~mask]\n",
        "            setattr(model, name, new_conv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zt4d-jci6N2"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def re_init_model_snip_ver2_little_grad(model, px): # re init smallest gradients\n",
        "    print(\"Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\")\n",
        "    for name, m in list(model.named_modules()):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            mask = torch.zeros_like(m.weight, device=DEVICE).bool()\n",
        "            nparams_toprune = round(px*mask.nelement())\n",
        "\n",
        "            out_c, in_c, ke, _ = mask.shape\n",
        "            value = -m.weight.grad.abs()\n",
        "            topk = torch.topk(value.view(-1), k=nparams_toprune)\n",
        "            mask.view(-1)[topk.indices] = True\n",
        "            grad_mask = mask.clone().float()\n",
        "            grad_mask[grad_mask==0] += 0.1\n",
        "\n",
        "            new_conv = MaskConv2d(grad_mask, m.in_channels, m.out_channels, m.kernel_size, m.stride,\n",
        "                 m.padding, m.dilation, m.groups, m.bias!=None, m.padding_mode, device=DEVICE)\n",
        "            nn.init.kaiming_normal_(new_conv.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "            new_conv.weight.data[~mask] = m.weight[~mask]\n",
        "\n",
        "            set_layer(model, name, new_conv)\n",
        "\n",
        "def set_layer(model, layer_name, layer):\n",
        "    splited = layer_name.split('.')\n",
        "    if len(splited) == 1:\n",
        "        setattr(model, splited[0], layer)\n",
        "    elif len(splited) == 3:\n",
        "        setattr(getattr(model, splited[0])[int(splited[1])], splited[2], layer)\n",
        "    elif len(splited) == 4:\n",
        "        getattr(getattr(model, splited[0])[int(splited[1])], splited[2])[int(splited[3])] = layer\n",
        "\n",
        "@torch.no_grad()\n",
        "def replace_maskconv(model):\n",
        "    print(\"Remove Maskconv\")\n",
        "    for name, m in list(model.named_modules()):\n",
        "        if isinstance(m, MaskConv2d):\n",
        "            conv = nn.Conv2d(m.in_channels, m.out_channels, m.kernel_size, m.stride,\n",
        "                 m.padding, m.dilation, m.groups, m.bias!=None, m.padding_mode, device=DEVICE)\n",
        "            conv.weight.data = m.weight\n",
        "            conv.bias = m.bias\n",
        "            set_layer(model, name, conv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMkLyYnUiWTm"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def re_init_model_snip_ver2(model, px): # re init smallest gradients\n",
        "    print(\"Apply Unstructured snip ve2 re init no grads Globally (all conv layers)\")\n",
        "    for name, m in model.named_modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            mask = torch.zeros_like(m.weight, device=DEVICE).bool()\n",
        "            nparams_toprune = round(px*mask.nelement())\n",
        "\n",
        "            out_c, in_c, ke, _ = mask.shape\n",
        "            value = -m.weight.grad.abs()\n",
        "            topk = torch.topk(value.view(-1), k=nparams_toprune)\n",
        "            mask.view(-1)[topk.indices] = True\n",
        "\n",
        "            m.weight.data[mask] = nn.Conv2d(in_c, out_c, ke, device=DEVICE).weight[mask]\n",
        "\n",
        "def get_grads_for_snip(model, retain_loader, forget_loader):\n",
        "    indices = torch.randperm(len(retain_loader.dataset), dtype=torch.int32, device='cpu')[:len(forget_loader.dataset)]\n",
        "\n",
        "    model.zero_grad()\n",
        "    for inputs, targets in retain_loader:\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "    for inputs, targets in  forget_loader:\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = -F.cross_entropy(outputs, targets)\n",
        "        loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Gb0GVoDh2dd"
      },
      "outputs": [],
      "source": [
        "def kookmin(\n",
        "    net,\n",
        "    retain_loader,\n",
        "    forget_loader,\n",
        "    val_loader):\n",
        "    if init_method=='snip_little_grad':\n",
        "        replace_maskconv(net)\n",
        "        get_grads_for_snip(net, retain_loader, forget_loader)\n",
        "        re_init_model_snip_ver2_little_grad(net, init_rate)\n",
        "    else:\n",
        "        raise \"not implemented\"\n",
        "\n",
        "    \"\"\"Simple unlearning by finetuning.\"\"\"\n",
        "    epochs = epoch\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr,\n",
        "                      momentum=0.9, weight_decay=weight_decay)\n",
        "    if sch=='cosine':\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "    elif sch=='linear':\n",
        "        scheduler = LinearAnnealingLR(optimizer, num_annealing_steps=(epochs+1)//2, num_total_steps=epochs+1)\n",
        "    elif sch=='decrease':\n",
        "        print('decrease')\n",
        "        scheduler = LinearAnnealingLR(optimizer, num_annealing_steps=1, num_total_steps=epochs+1)\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        for inputs, targets in retain_loader:\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "    #remove_prune(net)\n",
        "    net.eval()\n",
        "    print(\"Kookmin finished\")\n",
        "    return net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lukFE71wRa4"
      },
      "source": [
        "# Seif - 3rd place"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMKMSAEVwXOy"
      },
      "outputs": [],
      "source": [
        "def seif(\n",
        "    net,\n",
        "    retain_loader,\n",
        "    forget_loader,\n",
        "    val_loader):\n",
        "    \"\"\"Simple unlearning by finetuning.\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    class CustomCrossEntropyLoss(nn.Module):\n",
        "        def __init__(self, class_weights=None):\n",
        "            super(CustomCrossEntropyLoss, self).__init__()\n",
        "            self.class_weights = class_weights\n",
        "\n",
        "        def forward(self, input, target):\n",
        "            # Compute the standard cross-entropy loss\n",
        "            ce_loss = nn.functional.cross_entropy(input, target)\n",
        "\n",
        "            # Apply class weights to the loss if provided\n",
        "            if self.class_weights is not None:\n",
        "                # Calculate the weights for each element in the batch based on the target\n",
        "                weights = torch.tensor([self.class_weights[i] for i in target], device=input.device)\n",
        "                ce_loss = torch.mean(ce_loss * weights)\n",
        "\n",
        "            return ce_loss\n",
        "\n",
        "\n",
        "\n",
        "    # Define the vision_confuser function\n",
        "    def vision_confuser(model, std = 0.6):\n",
        "        for name, module in model.named_children():\n",
        "            if hasattr(module, 'weight'):\n",
        "                if 'conv' in name:\n",
        "\n",
        "                    actual_value = module.weight.clone().detach()\n",
        "                    new_values = torch.normal(mean=actual_value, std=std)\n",
        "                    module.weight.data.copy_(new_values)\n",
        "\n",
        "    vision_confuser(net)\n",
        "\n",
        "    epochs = 4\n",
        "\n",
        "    w = 0.05\n",
        "\n",
        "    class_weights = [1, w, w, w, w, w, w, w, w, w]\n",
        "    criterion = CustomCrossEntropyLoss(class_weights)\n",
        "\n",
        "\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.0007,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=epochs)\n",
        "\n",
        "    net.train()\n",
        "    i=0\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        i=0\n",
        "        net.train()\n",
        "        for inputs, targets in retain_loader:\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if (ep == epochs-2):\n",
        "            vision_confuser(net , 0.005) # increase model robustness before last training epoch\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    net.eval()\n",
        "    print(\"Seif finished\")\n",
        "    return net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyY_ieK1yo7q"
      },
      "source": [
        "# Sebastian - 4th place"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAsoEZagysGs"
      },
      "outputs": [],
      "source": [
        "def kl_loss_fn(outputs, dist_target):\n",
        "    kl_loss = F.kl_div(torch.log_softmax(outputs, dim=1), dist_target, log_target=True, reduction='batchmean')\n",
        "    return kl_loss\n",
        "\n",
        "def entropy_loss_fn(outputs, labels, dist_target, class_weights):\n",
        "    ce_loss = F.cross_entropy(outputs, labels, weight=class_weights)\n",
        "    entropy_dist_target = torch.sum(-torch.exp(dist_target) * dist_target, dim=1)\n",
        "    entropy_outputs = torch.sum(-torch.softmax(outputs, dim=1) * torch.log_softmax(outputs, dim=1), dim=1)\n",
        "    entropy_loss = F.mse_loss(entropy_outputs, entropy_dist_target)\n",
        "    return ce_loss + entropy_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-X5nlEkzdbe"
      },
      "outputs": [],
      "source": [
        "def sebastian(\n",
        "    net,\n",
        "    retain_loader,\n",
        "    forget_loader,\n",
        "    val_loader,\n",
        "    class_weights=None,\n",
        "):\n",
        "    \"\"\"Simple unlearning by finetuning.\"\"\"\n",
        "    epochs = 3.2\n",
        "    max_iters = int(len(retain_loader) * epochs)\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.0005,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "    initial_net = deepcopy(net)\n",
        "\n",
        "    net.train()\n",
        "    initial_net.eval()\n",
        "\n",
        "    def prune_model(net, amount=0.95, rand_init=True):\n",
        "        # Modules to prune\n",
        "        modules = list()\n",
        "        for k, m in enumerate(net.modules()):\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                modules.append((m, 'weight'))\n",
        "                if m.bias is not None:\n",
        "                    modules.append((m, 'bias'))\n",
        "\n",
        "        # Prune criteria\n",
        "        prune.global_unstructured(\n",
        "            modules,\n",
        "            #pruning_method=prune.RandomUnstructured,\n",
        "            pruning_method=prune.L1Unstructured,\n",
        "            amount=amount,\n",
        "        )\n",
        "\n",
        "        # Perform the prune\n",
        "        for k, m in enumerate(net.modules()):\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                prune.remove(m, 'weight')\n",
        "                if m.bias is not None:\n",
        "                    prune.remove(m, 'bias')\n",
        "\n",
        "        # Random initialization\n",
        "        if rand_init:\n",
        "            for k, m in enumerate(net.modules()):\n",
        "                if isinstance(m, nn.Conv2d):\n",
        "                    mask = m.weight == 0\n",
        "                    c_in = mask.shape[1]\n",
        "                    k = 1/(c_in*mask.shape[2]*mask.shape[3])\n",
        "                    randinit = (torch.rand_like(m.weight)-0.5)*2*sqrt(k)\n",
        "                    m.weight.data[mask] = randinit[mask]\n",
        "                if isinstance(m, nn.Linear):\n",
        "                    mask = m.weight == 0\n",
        "                    c_in = mask.shape[1]\n",
        "                    k = 1/c_in\n",
        "                    randinit = (torch.rand_like(m.weight)-0.5)*2*sqrt(k)\n",
        "                    m.weight.data[mask] = randinit[mask]\n",
        "\n",
        "    num_iters = 0\n",
        "    running = True\n",
        "    prune_amount = 0.99\n",
        "    prune_model(net, prune_amount, True)\n",
        "    while running:\n",
        "        net.train()\n",
        "        for inputs, targets in retain_loader:\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "            # Get target distribution\n",
        "            with torch.no_grad():\n",
        "                original_outputs = initial_net(inputs)\n",
        "                preds = torch.log_softmax(original_outputs, dim=1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = entropy_loss_fn(outputs, targets, preds, class_weights)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            num_iters += 1\n",
        "            # Stop at max iters\n",
        "            if num_iters > max_iters:\n",
        "                running = False\n",
        "                break\n",
        "\n",
        "    net.eval()\n",
        "    print(\"Sebastian finished\")\n",
        "    return net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SkJ-KYR5G1O"
      },
      "source": [
        "# Amnesiacs - 6th place"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ81ZzKzBsk3"
      },
      "source": [
        "# SHAP values and Accuracy function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ye3mn_q0BvXy"
      },
      "outputs": [],
      "source": [
        "def calculate_shap(model, test_set):\n",
        "    images = [test_set[i][0] for i in range(len(test_set))]\n",
        "    class_0_images = [test_set[i][0] for i in range(len(test_set)) if test_set[i][1] == 0]\n",
        "    class_1_images = [test_set[i][0] for i in range(len(test_set)) if test_set[i][1] == 1]\n",
        "    class_2_images = [test_set[i][0] for i in range(len(test_set)) if test_set[i][1] == 2]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()  # Tryb ewaluacji\n",
        "\n",
        "    # Konwersja listy obrazów na tensor i przeniesienie na CPU\n",
        "    def preprocess(images):\n",
        "        images = torch.stack(images).cpu().numpy()  # (batch, C, H, W)\n",
        "        images = np.transpose(images, (0, 2, 3, 1))  # (batch, H, W, C)\n",
        "        images = (images * 255).astype(np.uint8)  # Konwersja do uint8\n",
        "        return images\n",
        "\n",
        "    test_images = preprocess(images)\n",
        "    class_0_images = preprocess(class_0_images)\n",
        "    class_1_images = preprocess(class_1_images)\n",
        "    class_2_images = preprocess(class_2_images)\n",
        "\n",
        "    def model_predict(input_data):\n",
        "        \"\"\"Przetwarzanie wsadami (batch-wise) dla SHAP\"\"\"\n",
        "        input_tensor = torch.tensor(input_data, dtype=torch.float32).to(device) / 255.0\n",
        "        input_tensor = input_tensor.permute(0, 3, 1, 2)  # (batch, C, H, W)\n",
        "\n",
        "        batch_size = 128\n",
        "        outputs = []\n",
        "\n",
        "        for i in range(0, len(input_tensor), batch_size):\n",
        "            batch = input_tensor[i:i + batch_size]\n",
        "            with torch.no_grad():\n",
        "                output = model(batch)\n",
        "                probs = torch.nn.functional.softmax(output, dim=1)\n",
        "            outputs.append(probs.cpu().numpy())\n",
        "\n",
        "        return np.concatenate(outputs, axis=0)\n",
        "\n",
        "    # Tworzymy maskera SHAP\n",
        "    masker = shap.maskers.Image(\"inpaint_telea\", test_images[0].shape)\n",
        "\n",
        "    # Tworzymy Explainera\n",
        "    explainer = shap.Explainer(model_predict, masker)\n",
        "\n",
        "    # Obliczamy wartości SHAP dla każdej klasy osobno\n",
        "    shap_values0 = explainer(class_0_images[:100])\n",
        "    shap_mean0 = shap_values0.values.sum(axis=(0, 1, 2))\n",
        "\n",
        "    shap_values1 = explainer(class_1_images[:100])\n",
        "    shap_mean1 = shap_values1.values.sum(axis=(0, 1, 2))\n",
        "\n",
        "    \"\"\"shap_values2 = explainer(class_2_images)\n",
        "    shap_mean2 = shap_values2.values.sum(axis=(0, 1, 2))\"\"\"\n",
        "\n",
        "    return np.array([shap_mean0, shap_mean1])\n",
        "\n",
        "def shap_diffrence(previous_shap, next_shap):\n",
        "  diffrence_shap = np.mean(np.array(next_shap) - np.array(previous_shap))\n",
        "\n",
        "  return diffrence_shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S60boydAF1_a"
      },
      "outputs": [],
      "source": [
        "initial_shap_values = calculate_shap(model, test_set)\n",
        "print(initial_shap_values.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7LbdG0-9NW9"
      },
      "outputs": [],
      "source": [
        "print(initial_shap_values[0][0][0])\n",
        "print(initial_shap_values[0][1][0])\n",
        "print(initial_shap_values[1][0][0])\n",
        "print(initial_shap_values[1][1][0])\n",
        "#print(initial_shap_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFOdFfSnVJz1"
      },
      "outputs": [],
      "source": [
        "def accuracy(net, loader):\n",
        "    \"\"\"Return accuracy on a dataset given by the data loader.\"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, targets in loader:\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "print(f\"Train set accuracy: {100.0 * accuracy(model, train_loader):0.1f}%\")\n",
        "print(f\"Test set accuracy: {100.0 * accuracy(model, test_loader):0.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O3uDhuhyItp"
      },
      "source": [
        "# Unlearning: Scenario A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94r5nBsCrJUg"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "rt_model = copy.deepcopy(model)\n",
        "scenario_0_rt = []\n",
        "scenario_0_fau = []\n",
        "scenario_0_koo = []\n",
        "scenario_0_sei = []\n",
        "scenario_0_seb = []\n",
        "scenario_0_amn = []\n",
        "for i in range(5):\n",
        "  tmp_class_indices = class_indices.copy()\n",
        "  fau_model = copy.deepcopy(model)\n",
        "  koo_model = copy.deepcopy(model)\n",
        "  sei_model = copy.deepcopy(model)\n",
        "  seb_model = copy.deepcopy(model)\n",
        "  amn_model = copy.deepcopy(model)\n",
        "  forget_loader, retain_loader, tmp_class_indices = data_to_forget(train_set, tmp_class_indices, 0, 5000)\n",
        "  print(\"Fauchan\")\n",
        "  fau_model = fauchan(fau_model, retain_loader, forget_loader, val_loader)\n",
        "  print(\"Kookmin\")\n",
        "  koo_model = kookmin(koo_model, retain_loader, forget_loader, val_loader)\n",
        "  print(\"Seif\")\n",
        "  sei_model = seif(sei_model, retain_loader, forget_loader, val_loader)\n",
        "  print(\"Sebastian\")\n",
        "  seb_model = sebastian(seb_model, retain_loader, forget_loader, val_loader)\n",
        "  print(\"Amnesiacs\")\n",
        "  amn_model = amnesiacs(amn_model, retain_loader, forget_loader, val_loader)\n",
        "  print(\"Full retraining\")\n",
        "  rt_model = full_retraining(retain_loader)\n",
        "  koo_shap = calculate_shap(koo_model, test_set)\n",
        "  sei_shap = calculate_shap(sei_model, test_set)\n",
        "  seb_shap = calculate_shap(seb_model, test_set)\n",
        "  amn_shap = calculate_shap(amn_model, test_set)\n",
        "  rt_shap = calculate_shap(rt_model, test_set)\n",
        "  fau_shap = calculate_shap(fau_model, test_set)\n",
        "  accuracy_rt_all = accuracy(rt_model, test_loader)\n",
        "  accuracy_fau_all = accuracy(fau_model, test_loader)\n",
        "  accuracy_koo_all = accuracy(koo_model, test_loader)\n",
        "  accuracy_sei_all = accuracy(sei_model, test_loader)\n",
        "  accuracy_seb_all = accuracy(seb_model, test_loader)\n",
        "  accuracy_amn_all = accuracy(amn_model, test_loader)\n",
        "  accuracy_rt_0 = accuracy(rt_model, test_loader_0)\n",
        "  accuracy_fau_0 = accuracy(fau_model, test_loader_0)\n",
        "  accuracy_koo_0 = accuracy(koo_model, test_loader_0)\n",
        "  accuracy_sei_0 = accuracy(sei_model, test_loader_0)\n",
        "  accuracy_seb_0 = accuracy(seb_model, test_loader_0)\n",
        "  accuracy_amn_0 = accuracy(amn_model, test_loader_0)\n",
        "  accuracy_rt_no_0 = accuracy(rt_model, test_loader_no_0)\n",
        "  accuracy_fau_no_0 = accuracy(fau_model, test_loader_no_0)\n",
        "  accuracy_koo_no_0 = accuracy(koo_model, test_loader_no_0)\n",
        "  accuracy_sei_no_0 = accuracy(sei_model, test_loader_no_0)\n",
        "  accuracy_seb_no_0 = accuracy(seb_model, test_loader_no_0)\n",
        "  accuracy_amn_no_0 = accuracy(amn_model, test_loader_no_0)\n",
        "  scenario_0_rt.append((accuracy_rt_all, accuracy_rt_0, accuracy_rt_no_0))\n",
        "  scenario_0_fau.append((accuracy_fau_all, accuracy_fau_0, accuracy_fau_no_0))\n",
        "  scenario_0_koo.append((accuracy_koo_all, accuracy_koo_0, accuracy_koo_no_0))\n",
        "  scenario_0_sei.append((accuracy_sei_all, accuracy_sei_0, accuracy_sei_no_0))\n",
        "  scenario_0_seb.append((accuracy_seb_all, accuracy_seb_0, accuracy_seb_no_0))\n",
        "  scenario_0_amn.append((accuracy_amn_all, accuracy_amn_0, accuracy_amn_no_0))\n",
        "print(scenario_0_rt)\n",
        "print(scenario_0_fau)\n",
        "print(scenario_0_koo)\n",
        "print(scenario_0_sei)\n",
        "print(scenario_0_seb)\n",
        "print(scenario_0_amn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGxVgEyipUF7"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "rt_model = copy.deepcopy(model)\n",
        "fau_model = copy.deepcopy(model)\n",
        "koo_model = copy.deepcopy(model)\n",
        "sei_model = copy.deepcopy(model)\n",
        "seb_model = copy.deepcopy(model)\n",
        "amn_model = copy.deepcopy(model)\n",
        "tmp_class_indices = class_indices.copy()\n",
        "scenario_0_rt = []\n",
        "scenario_0_fau = []\n",
        "scenario_0_koo = []\n",
        "scenario_0_sei = []\n",
        "scenario_0_seb = []\n",
        "scenario_0_amn = []\n",
        "\n",
        "for i in range(50):\n",
        "  forget_loader, retain_loader, tmp_class_indices = data_to_forget(train_set, tmp_class_indices, 0, 100)\n",
        "  print(\"Fauchan\")\n",
        "  fau_model = fauchan(fau_model, retain_loader, forget_loader, val_loader)\n",
        "  print(\"Kookmin\")\n",
        "  koo_model = kookmin(koo_model, retain_loader, forget_loader, val_loader)\n",
        "  print(\"Seif\")\n",
        "  sei_model = seif(sei_model, retain_loader, forget_loader, val_loader)\n",
        "  print(\"Sebastian\")\n",
        "  seb_model = sebastian(seb_model, retain_loader, forget_loader, val_loader)\n",
        "  print(\"Amnesiacs\")\n",
        "  amn_model = amnesiacs(amn_model, retain_loader, forget_loader, val_loader)\n",
        "  print(\"Full retraining\")\n",
        "  rt_model = full_retraining(retain_loader)\n",
        "  koo_shap = calculate_shap(koo_model, test_set)\n",
        "  sei_shap = calculate_shap(sei_model, test_set)\n",
        "  seb_shap = calculate_shap(seb_model, test_set)\n",
        "  amn_shap = calculate_shap(amn_model, test_set)\n",
        "  rt_shap = calculate_shap(rt_model, test_set)\n",
        "  fau_shap = calculate_shap(fau_model, test_set)\n",
        "  accuracy_rt_all = accuracy(rt_model, test_loader)\n",
        "  accuracy_fau_all = accuracy(fau_model, test_loader)\n",
        "  accuracy_koo_all = accuracy(koo_model, test_loader)\n",
        "  accuracy_sei_all = accuracy(sei_model, test_loader)\n",
        "  accuracy_seb_all = accuracy(seb_model, test_loader)\n",
        "  accuracy_amn_all = accuracy(amn_model, test_loader)\n",
        "  accuracy_rt_0 = accuracy(rt_model, test_loader_0)\n",
        "  accuracy_fau_0 = accuracy(fau_model, test_loader_0)\n",
        "  accuracy_koo_0 = accuracy(koo_model, test_loader_0)\n",
        "  accuracy_sei_0 = accuracy(sei_model, test_loader_0)\n",
        "  accuracy_seb_0 = accuracy(seb_model, test_loader_0)\n",
        "  accuracy_amn_0 = accuracy(amn_model, test_loader_0)\n",
        "  accuracy_rt_no_0 = accuracy(rt_model, test_loader_no_0)\n",
        "  accuracy_fau_no_0 = accuracy(fau_model, test_loader_no_0)\n",
        "  accuracy_koo_no_0 = accuracy(koo_model, test_loader_no_0)\n",
        "  accuracy_sei_no_0 = accuracy(sei_model, test_loader_no_0)\n",
        "  accuracy_seb_no_0 = accuracy(seb_model, test_loader_no_0)\n",
        "  accuracy_amn_no_0 = accuracy(amn_model, test_loader_no_0)\n",
        "  scenario_0_rt.append((accuracy_rt_all, accuracy_rt_0, accuracy_rt_no_0, rt_shap))\n",
        "  scenario_0_fau.append((accuracy_fau_all, accuracy_fau_0, accuracy_fau_no_0, fau_shap))\n",
        "  scenario_0_koo.append((accuracy_koo_all, accuracy_koo_0, accuracy_koo_no_0, koo_shap))\n",
        "  scenario_0_sei.append((accuracy_sei_all, accuracy_sei_0, accuracy_sei_no_0, sei_shap))\n",
        "  scenario_0_seb.append((accuracy_seb_all, accuracy_seb_0, accuracy_seb_no_0, seb_shap))\n",
        "  scenario_0_amn.append((accuracy_amn_all, accuracy_amn_0, accuracy_amn_no_0, amn_shap))\n",
        "  print(\"Finish\"+str(i))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unlearning: Scenario B"
      ],
      "metadata": {
        "id": "GVRjL7u0pHsg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZLLTnKeqaK_"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "rt_model = copy.deepcopy(model)\n",
        "fau_model = copy.deepcopy(model)\n",
        "koo_model = copy.deepcopy(model)\n",
        "sei_model = copy.deepcopy(model)\n",
        "seb_model = copy.deepcopy(model)\n",
        "amn_model = copy.deepcopy(model)\n",
        "tmp_class_indices = class_indices.copy()\n",
        "scenario_0_rt = []\n",
        "scenario_0_fau = []\n",
        "scenario_0_koo = []\n",
        "scenario_0_sei = []\n",
        "scenario_0_seb = []\n",
        "scenario_0_amn = []\n",
        "\n",
        "for i in range(50):\n",
        "  forget_loader, retain_loader, tmp_class_indices = data_to_forget(train_set, tmp_class_indices, -1, 100)\n",
        "  print(\"Fauchan\")\n",
        "  fau_model = fauchan(fau_model, retain_loader, forget_loader, val_loader)\n",
        "  print(\"Kookmin\")\n",
        "  koo_model = kookmin(koo_model, retain_loader, forget_loader, val_loader)\n",
        "  print(\"Seif\")\n",
        "  sei_model = seif(sei_model, retain_loader, forget_loader, val_loader)\n",
        "  print(\"Sebastian\")\n",
        "  seb_model = sebastian(seb_model, retain_loader, forget_loader, val_loader)\n",
        "  print(\"Amnesiacs\")\n",
        "  amn_model = amnesiacs(amn_model, retain_loader, forget_loader, val_loader)\n",
        "  print(\"Full retraining\")\n",
        "  rt_model = full_retraining(retain_loader)\n",
        "  koo_shap = calculate_shap(koo_model, test_set)\n",
        "  sei_shap = calculate_shap(sei_model, test_set)\n",
        "  seb_shap = calculate_shap(seb_model, test_set)\n",
        "  amn_shap = calculate_shap(amn_model, test_set)\n",
        "  rt_shap = calculate_shap(rt_model, test_set)\n",
        "  fau_shap = calculate_shap(fau_model, test_set)\n",
        "  accuracy_rt_all = accuracy(rt_model, test_loader)\n",
        "  accuracy_fau_all = accuracy(fau_model, test_loader)\n",
        "  accuracy_koo_all = accuracy(koo_model, test_loader)\n",
        "  accuracy_sei_all = accuracy(sei_model, test_loader)\n",
        "  accuracy_seb_all = accuracy(seb_model, test_loader)\n",
        "  accuracy_amn_all = accuracy(amn_model, test_loader)\n",
        "  accuracy_rt_0 = accuracy(rt_model, test_loader_0)\n",
        "  accuracy_fau_0 = accuracy(fau_model, test_loader_0)\n",
        "  accuracy_koo_0 = accuracy(koo_model, test_loader_0)\n",
        "  accuracy_sei_0 = accuracy(sei_model, test_loader_0)\n",
        "  accuracy_seb_0 = accuracy(seb_model, test_loader_0)\n",
        "  accuracy_amn_0 = accuracy(amn_model, test_loader_0)\n",
        "  accuracy_rt_no_0 = accuracy(rt_model, test_loader_no_0)\n",
        "  accuracy_fau_no_0 = accuracy(fau_model, test_loader_no_0)\n",
        "  accuracy_koo_no_0 = accuracy(koo_model, test_loader_no_0)\n",
        "  accuracy_sei_no_0 = accuracy(sei_model, test_loader_no_0)\n",
        "  accuracy_seb_no_0 = accuracy(seb_model, test_loader_no_0)\n",
        "  accuracy_amn_no_0 = accuracy(amn_model, test_loader_no_0)\n",
        "  scenario_0_rt.append((accuracy_rt_all, accuracy_rt_0, accuracy_rt_no_0, rt_shap))\n",
        "  scenario_0_fau.append((accuracy_fau_all, accuracy_fau_0, accuracy_fau_no_0, fau_shap))\n",
        "  scenario_0_koo.append((accuracy_koo_all, accuracy_koo_0, accuracy_koo_no_0, koo_shap))\n",
        "  scenario_0_sei.append((accuracy_sei_all, accuracy_sei_0, accuracy_sei_no_0, sei_shap))\n",
        "  scenario_0_seb.append((accuracy_seb_all, accuracy_seb_0, accuracy_seb_no_0, seb_shap))\n",
        "  scenario_0_amn.append((accuracy_amn_all, accuracy_amn_0, accuracy_amn_no_0, amn_shap))\n",
        "  print(\"Finish\"+str(i))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save results"
      ],
      "metadata": {
        "id": "admnBS_KpaOo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bkj5Rb6q3lIl"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpolP8Hk6Pv0"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "save_path = \"/content/drive/My Drive/PPRAI/rt0.pkl\"\n",
        "with open(save_path, \"wb\") as file:\n",
        "    pickle.dump(scenario_0_rt, file)\n",
        "print(\"Plik zapisany w:\", save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXbl_IvJ7HwQ"
      },
      "outputs": [],
      "source": [
        "save_path = \"/content/drive/My Drive/PPRAI/fau0.pkl\"\n",
        "with open(save_path, \"wb\") as file:\n",
        "    pickle.dump(scenario_0_fau, file)\n",
        "print(\"Plik zapisany w:\", save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppLR_Fad7Y4S"
      },
      "outputs": [],
      "source": [
        "save_path = \"/content/drive/My Drive/PPRAI/koo0.pkl\"\n",
        "with open(save_path, \"wb\") as file:\n",
        "    pickle.dump(scenario_0_koo, file)\n",
        "print(\"Plik zapisany w:\", save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3ikyg647h5A"
      },
      "outputs": [],
      "source": [
        "save_path = \"/content/drive/My Drive/PPRAI/sei0.pkl\"\n",
        "with open(save_path, \"wb\") as file:\n",
        "    pickle.dump(scenario_0_sei, file)\n",
        "print(\"Plik zapisany w:\", save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzYlWKTj7qHB"
      },
      "outputs": [],
      "source": [
        "save_path = \"/content/drive/My Drive/PPRAI/seb0.pkl\"\n",
        "with open(save_path, \"wb\") as file:\n",
        "    pickle.dump(scenario_0_seb, file)\n",
        "print(\"Plik zapisany w:\", save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05DI98pc7tWk"
      },
      "outputs": [],
      "source": [
        "save_path = \"/content/drive/My Drive/PPRAI/amn0.pkl\"\n",
        "with open(save_path, \"wb\") as file:\n",
        "    pickle.dump(scenario_0_amn, file)\n",
        "print(\"Plik zapisany w:\", save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6o1yCuM60xW"
      },
      "outputs": [],
      "source": [
        "with open(save_path, \"rb\") as file:\n",
        "    loaded_data = pickle.load(file)\n",
        "\n",
        "print(loaded_data[29])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "kXEWkDTTn4iO",
        "WsOIZhKChv2G",
        "6lukFE71wRa4",
        "XyY_ieK1yo7q"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}