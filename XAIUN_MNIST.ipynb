{"cells":[{"cell_type":"markdown","metadata":{"id":"rg6hMnlC3tdi"},"source":["# MNIST EXAMPLE"]},{"cell_type":"markdown","source":["# Instaling libraries"],"metadata":{"id":"PuTLJteTuw3C"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"PQnF0dGgNdI9"},"outputs":[],"source":["!pip install shap --upgrade"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V6YXq1oMM3bj"},"outputs":[],"source":["import os\n","import random\n","import requests\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import linear_model, model_selection\n","\n","import torch\n","from torch import nn\n","from torch import optim\n","from torch.utils.data import DataLoader\n","\n","import torchvision\n","from torchvision import transforms\n","from torchvision.utils import make_grid\n","from torchvision.models import resnet18\n","import torch.nn.functional as F\n","import torch.nn.utils.prune as prune\n","from copy import deepcopy\n","from math import sqrt\n","from torch.utils.data import Subset\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Running on device:\", DEVICE.upper())\n","\n","# manual random seed is used for dataset partitioning\n","# to ensure reproducible results across runs\n","RNG = torch.Generator().manual_seed(42)"]},{"cell_type":"markdown","metadata":{"id":"CF7zXiGLM3bk"},"source":["#  Download dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1PBoIzPjcNLV"},"outputs":[],"source":["# download and pre-process MNIST\n","normalize = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,))\n","])\n","\n","train_set = torchvision.datasets.MNIST(\n","    root=\"./data\", train=True, download=True, transform=normalize\n",")\n","train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n","\n","held_out = torchvision.datasets.MNIST(\n","    root=\"./data\", train=False, download=True, transform=normalize\n",")\n","test_set, val_set = torch.utils.data.random_split(held_out, [0.5, 0.5], generator=RNG)\n","test_loader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n","val_loader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)"]},{"cell_type":"markdown","source":["# Training original model"],"metadata":{"id":"2ySXebsBvBkn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Rn6UKX_M3bl"},"outputs":[],"source":["model = resnet18(pretrained=False)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Dostosowanie klasyfikatora do 10 klas (MNIST)\n","model = resnet18(pretrained=False)\n","model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)  # Zmiana liczby wejściowych kanałów na 1\n","model.fc = nn.Linear(model.fc.in_features, 10)\n","model = model.to(device)\n","\n","# 3. Definicja funkcji straty i optymalizatora\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# 4. Trenowanie modelu\n","num_epochs = 5\n","train_losses = []\n","test_accuracies = []\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    avg_loss = running_loss / len(train_loader)\n","    train_losses.append(avg_loss)\n","    print(f\"Epoka [{epoch+1}/{num_epochs}], Strata: {avg_loss:.4f}\")\n","\n","    # Ewaluacja na zbiorze testowym\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    test_accuracies.append(accuracy)\n","    print(f\"Dokładność na zbiorze testowym: {accuracy:.2f}%\")"]},{"cell_type":"markdown","source":["# Retrain function and accuracy"],"metadata":{"id":"6u_R8BVzvLFu"}},{"cell_type":"code","source":["def full_retrain(train_loader):\n","  model = resnet18(pretrained=False)\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","  model = resnet18(pretrained=False)\n","  model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","  model.fc = nn.Linear(model.fc.in_features, 10)\n","  model = model.to(device)\n","\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","  num_epochs = 5\n","  train_losses = []\n","  test_accuracies = []\n","\n","  for epoch in range(num_epochs):\n","      model.train()\n","      running_loss = 0.0\n","\n","      for images, labels in train_loader:\n","          images, labels = images.to(device), labels.to(device)\n","\n","          optimizer.zero_grad()\n","          outputs = model(images)\n","          loss = criterion(outputs, labels)\n","          loss.backward()\n","          optimizer.step()\n","\n","          running_loss += loss.item()\n","\n","      avg_loss = running_loss / len(train_loader)\n","      train_losses.append(avg_loss)\n","      print(f\"Epoka [{epoch+1}/{num_epochs}], Strata: {avg_loss:.4f}\")\n","\n","      # Ewaluacja na zbiorze testowym\n","  model.eval()\n","  return model"],"metadata":{"id":"xXAFCmvjM1HJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ttahi-1w1ai9"},"outputs":[],"source":["def accuracy(net, loader):\n","    \"\"\"Return accuracy on a dataset given by the data loader.\"\"\"\n","    correct = 0\n","    total = 0\n","    for inputs, targets in loader:\n","        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n","        outputs = net(inputs)\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","    return correct / total\n","\n","\n","print(f\"Train set accuracy: {100.0 * accuracy(model, train_loader):0.1f}%\")\n","print(f\"Test set accuracy: {100.0 * accuracy(model, test_loader):0.1f}%\")"]},{"cell_type":"markdown","metadata":{"id":"B9OMjLg2zlq4"},"source":["# Forgetset split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uc32Ytc2xTH4"},"outputs":[],"source":["def data_to_forget(train, class_indices, label, size):\n","  sampled_class = []\n","  if label == -1:\n","    for i in range(10):\n","      print(\"*\")\n","      tmp = random.sample(class_indices[i], int(size/10))\n","      class_indices[i] = list(set(class_indices[i]) - set(tmp))\n","      sampled_class.extend(tmp)\n","  else:\n","    sampled_class = random.sample(class_indices[label], size)  # Wylosowanie 50 próbek\n","    class_indices[label] = list(set(class_indices[label]) - set(sampled_class))\n","\n","  flat_class_indices = [idx for sublist in class_indices for idx in sublist]\n","  forget_set = torch.utils.data.Subset(train_set, sampled_class)\n","  retain_set = torch.utils.data.Subset(train_set, flat_class_indices)\n","\n","  forget_loader = torch.utils.data.DataLoader(\n","      forget_set, batch_size=128, shuffle=True, num_workers=2\n","  )\n","  retain_loader = torch.utils.data.DataLoader(\n","      retain_set, batch_size=128, shuffle=True, num_workers=2, generator=RNG\n","  )\n","  return forget_loader, retain_loader, class_indices"]},{"cell_type":"markdown","source":["# Unlearning alghoritms\n","Source: https://www.kaggle.com/competitions/neurips-2023-machine-unlearning"],"metadata":{"id":"nltX8zs_vYHt"}},{"cell_type":"markdown","metadata":{"id":"kXEWkDTTn4iO"},"source":["#  Fauchan - 1st place\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"12BWO9ORUoyX"},"outputs":[],"source":["def evaluation(net, dataloader, criterion, device = 'cuda'): ##evaluation function\n","    net.eval()\n","    total_samp = 0\n","    total_acc = 0\n","    total_loss = 0.0\n","    for sample in dataloader:\n","        images, labels = sample['image'].to(device), sample['age_group'].to(device)\n","        _pred = net(images)\n","        total_samp+=len(labels)\n","        #print(f'total_samp={total_samp}')\n","        loss = criterion(_pred, labels)\n","        total_loss += loss.item()\n","        total_acc+=(_pred.max(1)[1] == labels).float().sum().item()\n","        #print(f'total_acc={total_acc}')\n","    #print(f'total_sample={total_samp}')\n","    mean_loss = total_loss / len(dataloader)\n","    mean_acc = total_acc/total_samp\n","    print(f'loss={mean_loss}')\n","    print(f'acc={mean_acc}')\n","    return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dvLkBLisM3bm"},"outputs":[],"source":["USE_MOCK: bool = False\n","\n","from torch.optim.lr_scheduler import CosineAnnealingLR,CosineAnnealingWarmRestarts,StepLR\n","def kl_loss_sym(x,y):\n","    kl_loss = nn.KLDivLoss(reduction='batchmean')\n","    return kl_loss(nn.LogSoftmax(dim=-1)(x),y)\n","def fauchan(\n","        net,\n","        retain_loader,\n","        forget_loader,\n","        val_loader,\n","):\n","    \"\"\"Simple unlearning by finetuning.\"\"\"\n","    print('-----------------------------------')\n","    epochs = 8\n","    retain_bs = 256\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.SGD(net.parameters(), lr=0.005,\n","                          momentum=0.9, weight_decay=0)\n","    optimizer_retain = optim.SGD(net.parameters(), lr=0.001*retain_bs/64, momentum=0.9, weight_decay=1e-2)\n","    ##the learning rate is associated with the batchsize we used\n","    optimizer_forget = optim.SGD(net.parameters(), lr=3e-4, momentum=0.9, weight_decay=0)\n","    total_step = int(len(forget_loader)*epochs)\n","    retain_ld = DataLoader(retain_loader.dataset, batch_size=retain_bs, shuffle=True)\n","    retain_ld4fgt = DataLoader(retain_loader.dataset, batch_size=256, shuffle=True)\n","    scheduler = CosineAnnealingLR(optimizer_forget, T_max=total_step, eta_min=1e-6)\n","    \"\"\"if USE_MOCK: ##Use some Local Metric as reference\n","        net.eval()\n","        print('Forget')\n","        evaluation(net, forget_loader, criterion)\n","        print('Valid')\n","        evaluation(net, validation_loader, criterion)\"\"\"\n","    net.train()\n","    for inputs, targets in forget_loader: ##First Stage\n","        inputs = inputs.to(DEVICE)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        uniform_label = torch.ones_like(outputs).to(DEVICE) / outputs.shape[1] ##uniform pseudo label\n","        loss = kl_loss_sym(outputs, uniform_label) ##optimize the distance between logits and pseudo labels\n","        loss.backward()\n","        optimizer.step()\n","    \"\"\"if USE_MOCK:\n","        print('Forget')\n","        evaluation(net,forget_loader,criterion)\n","        print('Valid')\n","        evaluation(net, validation_loader,criterion)\n","        print(f'epoch={epochs} and retain batch_sz={retain_bs}')\"\"\"\n","    net.train()\n","    for ep in range(epochs): ##Second Stage\n","        net.train()\n","        for (inputs_forget, outputs_forget), (inputs_retain, outputs_retain) in zip(forget_loader, retain_ld4fgt):##Forget Round\n","            t = 1.15 ##temperature coefficient\n","            inputs_forget, inputs_retain = inputs_forget.to(DEVICE), inputs_retain.to(DEVICE)\n","            optimizer_forget.zero_grad()\n","            outputs_forget,outputs_retain = net(inputs_forget),net(inputs_retain).detach()\n","            loss = (-1 * nn.LogSoftmax(dim=-1)(outputs_forget @ outputs_retain.T/t)).mean() ##Contrastive Learning loss\n","            loss.backward()\n","            optimizer_forget.step()\n","            scheduler.step()\n","        for inputs, labels in retain_ld: ##Retain Round\n","            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","            optimizer_retain.zero_grad()\n","            outputs = net(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer_retain.step()\n","        \"\"\"if USE_MOCK:\n","            print(f'epoch {ep}:')\n","            print('Retain')\n","            evaluation(net, retain_ld, criterion)\n","            print('Forget')\n","            evaluation(net, forget_loader, criterion)\n","            print('Valid')\n","            evaluation(net, validation_loader, criterion)\"\"\"\n","    print('-----------------------------------')\n","    return net"]},{"cell_type":"markdown","metadata":{"id":"WsOIZhKChv2G"},"source":["# Kookmin - 2 place"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RE4rsGYSh6eJ"},"outputs":[],"source":["sch = 'linear'\n","init_rate = 0.3\n","init_method = 'snip_little_grad'\n","lr = 0.001\n","epoch = 5\n","weight_decay = 5e-4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J4IYp9pxiKli"},"outputs":[],"source":["from torch.optim.lr_scheduler import _LRScheduler\n","\n","class LinearAnnealingLR(_LRScheduler):\n","    def __init__(self, optimizer, num_annealing_steps, num_total_steps):\n","        self.num_annealing_steps = num_annealing_steps\n","        self.num_total_steps = num_total_steps\n","\n","        super().__init__(optimizer)\n","\n","    def get_lr(self):\n","        if self._step_count <= self.num_annealing_steps:\n","            return [base_lr * self._step_count / self.num_annealing_steps for base_lr in self.base_lrs]\n","        else:\n","            return [base_lr * (self.num_total_steps - self._step_count) / (self.num_total_steps - self.num_annealing_steps) for base_lr in self.base_lrs]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EEK8_pTyi5L-"},"outputs":[],"source":["class Masker(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, x, mask):\n","        ctx.save_for_backward(mask)\n","        return x\n","\n","    @staticmethod\n","    def backward(ctx, grad):\n","        mask, = ctx.saved_tensors\n","        return grad * mask, None\n","\n","\n","class MaskConv2d(nn.Conv2d):\n","    def __init__(self, mask, in_channels, out_channels, kernel_size, stride=1,\n","                 padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device='cpu'):\n","        super(MaskConv2d, self).__init__(in_channels, out_channels, kernel_size, stride,\n","                                     padding, dilation, groups, bias, padding_mode, device=device)\n","        self.mask = mask\n","\n","    def forward(self, input):\n","        masked_weight = Masker.apply(self.weight, self.mask)\n","        return super(MaskConv2d, self)._conv_forward(input, masked_weight, self.bias)\n","\n","@torch.no_grad()\n","def re_init_model_random_zero_grad_Maskconv(model, px):\n","    print(\"Apply Unstructured random re init no grads Globally (all conv layers)\")\n","    for name, m in list(model.named_modules()):\n","        if isinstance(m, nn.Conv2d):\n","            mask = torch.zeros_like(m.weight, device=DEVICE).bool()\n","            nparams_toprune = round(px*mask.nelement())\n","\n","            prob = torch.rand_like(mask.float(), device=DEVICE)\n","            topk = torch.topk(prob.view(-1), k=nparams_toprune)\n","            mask.view(-1)[topk.indices] = True\n","\n","            new_conv = MaskConv2d(mask, m.in_channels, m.out_channels, m.kernel_size, m.stride,\n","                 m.padding, m.dilation, m.groups, m.bias!=None, m.padding_mode, device=DEVICE)\n","            #nn.init.kaiming_normal_(new_conv.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n","            new_conv.weight.data[~mask] = m.weight[~mask]\n","            setattr(model, name, new_conv)\n","\n","@torch.no_grad()\n","def re_init_model_random_little_grad_Maskconv_fanout(model, px):\n","    print(\"Apply Unstructured re_init_model_random_little_grad_Maskconv_fanout (all conv layers)\")\n","    for name, m in list(model.named_modules()):\n","        if isinstance(m, nn.Conv2d):\n","            mask = torch.zeros_like(m.weight, device=DEVICE).bool()\n","            nparams_toprune = round(px*mask.nelement())\n","\n","            prob = torch.rand_like(mask.float(), device=DEVICE)\n","            topk = torch.topk(prob.view(-1), k=nparams_toprune)\n","            mask.view(-1)[topk.indices] = True\n","            grad_mask = mask.clone().float()\n","            grad_mask[grad_mask==0] += 0.1\n","\n","            new_conv = MaskConv2d(grad_mask, m.in_channels, m.out_channels, m.kernel_size, m.stride,\n","                 m.padding, m.dilation, m.groups, m.bias!=None, m.padding_mode, device=DEVICE)\n","            nn.init.kaiming_normal_(new_conv.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n","            new_conv.weight.data[~mask] = m.weight[~mask]\n","            setattr(model, name, new_conv)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8zt4d-jci6N2"},"outputs":[],"source":["@torch.no_grad()\n","def re_init_model_snip_ver2_little_grad(model, px): # re init smallest gradients\n","    print(\"Apply Unstructured re_init_model_snip_ver2_little_grad Globally (all conv layers)\")\n","    for name, m in list(model.named_modules()):\n","        if isinstance(m, nn.Conv2d):\n","            mask = torch.zeros_like(m.weight, device=DEVICE).bool()\n","            nparams_toprune = round(px*mask.nelement())\n","\n","            out_c, in_c, ke, _ = mask.shape\n","            value = -m.weight.grad.abs()\n","            topk = torch.topk(value.view(-1), k=nparams_toprune)\n","            mask.view(-1)[topk.indices] = True\n","            grad_mask = mask.clone().float()\n","            grad_mask[grad_mask==0] += 0.1\n","\n","            new_conv = MaskConv2d(grad_mask, m.in_channels, m.out_channels, m.kernel_size, m.stride,\n","                 m.padding, m.dilation, m.groups, m.bias!=None, m.padding_mode, device=DEVICE)\n","            nn.init.kaiming_normal_(new_conv.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n","\n","            new_conv.weight.data[~mask] = m.weight[~mask]\n","\n","            set_layer(model, name, new_conv)\n","\n","def set_layer(model, layer_name, layer):\n","    splited = layer_name.split('.')\n","    if len(splited) == 1:\n","        setattr(model, splited[0], layer)\n","    elif len(splited) == 3:\n","        setattr(getattr(model, splited[0])[int(splited[1])], splited[2], layer)\n","    elif len(splited) == 4:\n","        getattr(getattr(model, splited[0])[int(splited[1])], splited[2])[int(splited[3])] = layer\n","\n","@torch.no_grad()\n","def replace_maskconv(model):\n","    print(\"Remove Maskconv\")\n","    for name, m in list(model.named_modules()):\n","        if isinstance(m, MaskConv2d):\n","            conv = nn.Conv2d(m.in_channels, m.out_channels, m.kernel_size, m.stride,\n","                 m.padding, m.dilation, m.groups, m.bias!=None, m.padding_mode, device=DEVICE)\n","            conv.weight.data = m.weight\n","            conv.bias = m.bias\n","            set_layer(model, name, conv)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kMkLyYnUiWTm"},"outputs":[],"source":["@torch.no_grad()\n","def re_init_model_snip_ver2(model, px): # re init smallest gradients\n","    print(\"Apply Unstructured snip ve2 re init no grads Globally (all conv layers)\")\n","    for name, m in model.named_modules():\n","        if isinstance(m, nn.Conv2d):\n","            mask = torch.zeros_like(m.weight, device=DEVICE).bool()\n","            nparams_toprune = round(px*mask.nelement())\n","\n","            out_c, in_c, ke, _ = mask.shape\n","            value = -m.weight.grad.abs()\n","            topk = torch.topk(value.view(-1), k=nparams_toprune)\n","            mask.view(-1)[topk.indices] = True\n","\n","            m.weight.data[mask] = nn.Conv2d(in_c, out_c, ke, device=DEVICE).weight[mask]\n","\n","def get_grads_for_snip(model, retain_loader, forget_loader):\n","    indices = torch.randperm(len(retain_loader.dataset), dtype=torch.int32, device='cpu')[:len(forget_loader.dataset)]\n","\n","    model.zero_grad()\n","    for inputs, targets in retain_loader:\n","        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n","\n","        outputs = model(inputs)\n","        loss = F.cross_entropy(outputs, targets)\n","        loss.backward()\n","\n","    for inputs, targets in  forget_loader:\n","        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n","\n","        outputs = model(inputs)\n","        loss = -F.cross_entropy(outputs, targets)\n","        loss.backward()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Gb0GVoDh2dd"},"outputs":[],"source":["def kookmin(\n","    net,\n","    retain_loader,\n","    forget_loader,\n","    val_loader):\n","    if init_method=='snip_little_grad':\n","        replace_maskconv(net)\n","        get_grads_for_snip(net, retain_loader, forget_loader)\n","        re_init_model_snip_ver2_little_grad(net, init_rate)\n","    else:\n","        raise \"not implemented\"\n","\n","    \"\"\"Simple unlearning by finetuning.\"\"\"\n","    epochs = epoch\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.SGD(net.parameters(), lr=lr,\n","                      momentum=0.9, weight_decay=weight_decay)\n","    if sch=='cosine':\n","        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n","    elif sch=='linear':\n","        scheduler = LinearAnnealingLR(optimizer, num_annealing_steps=(epochs+1)//2, num_total_steps=epochs+1)\n","    elif sch=='decrease':\n","        print('decrease')\n","        scheduler = LinearAnnealingLR(optimizer, num_annealing_steps=1, num_total_steps=epochs+1)\n","\n","    net.train()\n","\n","    for ep in range(epochs):\n","        for inputs, targets in retain_loader:\n","            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n","\n","            optimizer.zero_grad()\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","        scheduler.step()\n","    #remove_prune(net)\n","    net.eval()\n","    return net"]},{"cell_type":"markdown","metadata":{"id":"6lukFE71wRa4"},"source":["# Seif - 3rd place"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eMKMSAEVwXOy"},"outputs":[],"source":["def seif(\n","    net,\n","    retain_loader,\n","    forget_loader,\n","    val_loader):\n","    \"\"\"Simple unlearning by finetuning.\"\"\"\n","\n","\n","\n","    class CustomCrossEntropyLoss(nn.Module):\n","        def __init__(self, class_weights=None):\n","            super(CustomCrossEntropyLoss, self).__init__()\n","            self.class_weights = class_weights\n","\n","        def forward(self, input, target):\n","            # Compute the standard cross-entropy loss\n","            ce_loss = nn.functional.cross_entropy(input, target)\n","\n","            # Apply class weights to the loss if provided\n","            if self.class_weights is not None:\n","                # Calculate the weights for each element in the batch based on the target\n","                weights = torch.tensor([self.class_weights[i] for i in target], device=input.device)\n","                ce_loss = torch.mean(ce_loss * weights)\n","\n","            return ce_loss\n","\n","\n","\n","    # Define the vision_confuser function\n","    def vision_confuser(model, std = 0.6):\n","        for name, module in model.named_children():\n","            if hasattr(module, 'weight'):\n","                if 'conv' in name:\n","\n","                    actual_value = module.weight.clone().detach()\n","                    new_values = torch.normal(mean=actual_value, std=std)\n","                    module.weight.data.copy_(new_values)\n","\n","    vision_confuser(net)\n","\n","    epochs = 4\n","\n","    w = 0.05\n","\n","    class_weights = [1, w, w, w, w, w, w, w, w, w]\n","    criterion = CustomCrossEntropyLoss(class_weights)\n","\n","\n","    optimizer = optim.SGD(net.parameters(), lr=0.0007,\n","                      momentum=0.9, weight_decay=5e-4)\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n","        optimizer, T_max=epochs)\n","\n","    net.train()\n","    i=0\n","\n","    for ep in range(epochs):\n","        i=0\n","        net.train()\n","        for inputs, targets in retain_loader:\n","            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n","\n","            optimizer.zero_grad()\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","            loss.backward()\n","            optimizer.step()\n","\n","        if (ep == epochs-2):\n","            vision_confuser(net , 0.005) # increase model robustness before last training epoch\n","\n","        scheduler.step()\n","\n","    net.eval()\n","    return net"]},{"cell_type":"markdown","metadata":{"id":"XyY_ieK1yo7q"},"source":["# Sebastian - 4th place"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SAsoEZagysGs"},"outputs":[],"source":["def kl_loss_fn(outputs, dist_target):\n","    kl_loss = F.kl_div(torch.log_softmax(outputs, dim=1), dist_target, log_target=True, reduction='batchmean')\n","    return kl_loss\n","\n","def entropy_loss_fn(outputs, labels, dist_target, class_weights):\n","    ce_loss = F.cross_entropy(outputs, labels, weight=class_weights)\n","    entropy_dist_target = torch.sum(-torch.exp(dist_target) * dist_target, dim=1)\n","    entropy_outputs = torch.sum(-torch.softmax(outputs, dim=1) * torch.log_softmax(outputs, dim=1), dim=1)\n","    entropy_loss = F.mse_loss(entropy_outputs, entropy_dist_target)\n","    return ce_loss + entropy_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4-X5nlEkzdbe"},"outputs":[],"source":["def sebastian(\n","    net,\n","    retain_loader,\n","    forget_loader,\n","    val_loader,\n","    class_weights=None,\n","):\n","    \"\"\"Simple unlearning by finetuning.\"\"\"\n","    epochs = 3.2\n","    max_iters = int(len(retain_loader) * epochs)\n","    optimizer = optim.SGD(net.parameters(), lr=0.0005,\n","                      momentum=0.9, weight_decay=5e-4)\n","    initial_net = deepcopy(net)\n","\n","    net.train()\n","    initial_net.eval()\n","\n","    def prune_model(net, amount=0.95, rand_init=True):\n","        # Modules to prune\n","        modules = list()\n","        for k, m in enumerate(net.modules()):\n","            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n","                modules.append((m, 'weight'))\n","                if m.bias is not None:\n","                    modules.append((m, 'bias'))\n","\n","        # Prune criteria\n","        prune.global_unstructured(\n","            modules,\n","            #pruning_method=prune.RandomUnstructured,\n","            pruning_method=prune.L1Unstructured,\n","            amount=amount,\n","        )\n","\n","        # Perform the prune\n","        for k, m in enumerate(net.modules()):\n","            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n","                prune.remove(m, 'weight')\n","                if m.bias is not None:\n","                    prune.remove(m, 'bias')\n","\n","        # Random initialization\n","        if rand_init:\n","            for k, m in enumerate(net.modules()):\n","                if isinstance(m, nn.Conv2d):\n","                    mask = m.weight == 0\n","                    c_in = mask.shape[1]\n","                    k = 1/(c_in*mask.shape[2]*mask.shape[3])\n","                    randinit = (torch.rand_like(m.weight)-0.5)*2*sqrt(k)\n","                    m.weight.data[mask] = randinit[mask]\n","                if isinstance(m, nn.Linear):\n","                    mask = m.weight == 0\n","                    c_in = mask.shape[1]\n","                    k = 1/c_in\n","                    randinit = (torch.rand_like(m.weight)-0.5)*2*sqrt(k)\n","                    m.weight.data[mask] = randinit[mask]\n","\n","    num_iters = 0\n","    running = True\n","    prune_amount = 0.99\n","    prune_model(net, prune_amount, True)\n","    while running:\n","        net.train()\n","        for inputs, targets in retain_loader:\n","            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n","\n","            # Get target distribution\n","            with torch.no_grad():\n","                original_outputs = initial_net(inputs)\n","                preds = torch.log_softmax(original_outputs, dim=1)\n","\n","            optimizer.zero_grad()\n","            outputs = net(inputs)\n","            loss = entropy_loss_fn(outputs, targets, preds, class_weights)\n","            loss.backward()\n","            optimizer.step()\n","\n","            num_iters += 1\n","            # Stop at max iters\n","            if num_iters > max_iters:\n","                running = False\n","                break\n","\n","    net.eval()\n","    return net"]},{"cell_type":"markdown","metadata":{"id":"4SkJ-KYR5G1O"},"source":["# Amnesiacs - 6th place"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TCOcnmPw5F7h"},"outputs":[],"source":["def kl_loss(model_logits, teacher_logits, temperature=1.):\n","    '''\n","    Calculate the Kullback-Leibler (KL) divergence loss between the output of a model (student) and the output of a teacher model.\n","\n","    Args:\n","        model_logits (torch.Tensor): The logits from the student model. Logits are the raw outputs of the last neural network layer prior to softmax activation.\n","        teacher_logits (torch.Tensor): The logits from the teacher model.\n","        temperature (float, optional): A temperature parameter that scales the logits before applying softmax. Higher temperatures produce softer probability distributions. Defaults to 1.\n","\n","    Returns:\n","        torch.Tensor: The KL divergence loss, averaged over the batch.\n","    '''\n","    # apply softmax to the (scaled) logits of the teacher model\n","    teacher_output_softmax = F.softmax(teacher_logits / temperature, dim=1)\n","    # apply log softmax to the (scaled) logits of the student model\n","    output_log_softmax = F.log_softmax(model_logits / temperature, dim=1)\n","\n","    # calculate the KL divergence between the student and teacher outputs\n","    kl_div = F.kl_div(output_log_softmax, teacher_output_softmax, reduction='batchmean')\n","    return kl_div"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IBUe8yDm5gCx"},"outputs":[],"source":["def amnesiacs(\n","    net,\n","    retain_loader,\n","    forget_loader,\n","    val_loader,\n","    class_weights=None,\n","):\n","    \"\"\"Simple unlearning by finetuning.\"\"\"\n","    epochs = 3.2\n","    max_iters = int(len(retain_loader) * epochs)\n","    optimizer = optim.SGD(net.parameters(), lr=0.0005,\n","                      momentum=0.9, weight_decay=5e-4)\n","    initial_net = deepcopy(net)\n","\n","    net.train()\n","    initial_net.eval()\n","\n","    def prune_model(net, amount=0.95, rand_init=True):\n","        # Modules to prune\n","        modules = list()\n","        for k, m in enumerate(net.modules()):\n","            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n","                modules.append((m, 'weight'))\n","                if m.bias is not None:\n","                    modules.append((m, 'bias'))\n","\n","        # Prune criteria\n","        prune.global_unstructured(\n","            modules,\n","            #pruning_method=prune.RandomUnstructured,\n","            pruning_method=prune.L1Unstructured,\n","            amount=amount,\n","        )\n","\n","        # Perform the prune\n","        for k, m in enumerate(net.modules()):\n","            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n","                prune.remove(m, 'weight')\n","                if m.bias is not None:\n","                    prune.remove(m, 'bias')\n","\n","        # Random initialization\n","        if rand_init:\n","            for k, m in enumerate(net.modules()):\n","                if isinstance(m, nn.Conv2d):\n","                    mask = m.weight == 0\n","                    c_in = mask.shape[1]\n","                    k = 1/(c_in*mask.shape[2]*mask.shape[3])\n","                    randinit = (torch.rand_like(m.weight)-0.5)*2*sqrt(k)\n","                    m.weight.data[mask] = randinit[mask]\n","                if isinstance(m, nn.Linear):\n","                    mask = m.weight == 0\n","                    c_in = mask.shape[1]\n","                    k = 1/c_in\n","                    randinit = (torch.rand_like(m.weight)-0.5)*2*sqrt(k)\n","                    m.weight.data[mask] = randinit[mask]\n","\n","    num_iters = 0\n","    running = True\n","    prune_amount = 0.99\n","    prune_model(net, prune_amount, True)\n","    while running:\n","        net.train()\n","        for inputs, targets in retain_loader:\n","            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n","\n","            # Get target distribution\n","            with torch.no_grad():\n","                original_outputs = initial_net(inputs)\n","                preds = torch.log_softmax(original_outputs, dim=1)\n","\n","            optimizer.zero_grad()\n","            outputs = net(inputs)\n","            loss = entropy_loss_fn(outputs, targets, preds, class_weights)\n","            loss.backward()\n","            optimizer.step()\n","\n","            num_iters += 1\n","            # Stop at max iters\n","            if num_iters > max_iters:\n","                running = False\n","                break\n","\n","    net.eval()\n","    return net"]},{"cell_type":"markdown","metadata":{"id":"2O3uDhuhyItp"},"source":["# Unlearning operation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZZLLTnKeqaK_"},"outputs":[],"source":["import copy\n","\n","#rt_model = copy.deepcopy(model)\n","fau_model = copy.deepcopy(model)\n","koo_model = copy.deepcopy(model)\n","sei_model = copy.deepcopy(model)\n","seb_model = copy.deepcopy(model)\n","amn_model = copy.deepcopy(model)\n","\n","class_indices = [[] for _ in range(10)]\n","\n","for idx, (_, label) in enumerate(train_set):\n","    class_indices[label].append(idx)\n","\n","print(class_indices)\n","\n","forget_loader, retain_loader, class_indices = data_to_forget(train_set, class_indices, -1, 6000)\n","print(\"Fauchan\")\n","fau_model = fauchan(fau_model, retain_loader, forget_loader, val_loader)\n","print(\"Kookmin\")\n","koo_model = kookmin(koo_model, retain_loader, forget_loader, val_loader)\n","print(\"Seif\")\n","sei_model = seif(sei_model, retain_loader, forget_loader, val_loader)\n","print(\"Sebastian\")\n","seb_model = sebastian(seb_model, retain_loader, forget_loader, val_loader)\n","print(\"Amnesiacs\")\n","amn_model = amnesiacs(amn_model, retain_loader, forget_loader, val_loader)\n","print(\"Retain\")\n","rt_model = full_retrain(retain_loader)"]},{"cell_type":"markdown","metadata":{"id":"1--PInKr9-6B"},"source":["# SHAP Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9GoWN5vt3Lwo"},"outputs":[],"source":["import shap\n","from collections import defaultdict\n","import torch\n","\n","def calculate_shap(model, test_images):\n","  images = test_images\n","  images = images.to(DEVICE)\n","\n","  explainer = shap.GradientExplainer(model, images)\n","\n","  shap_values = explainer.shap_values(images)\n","\n","  shap_values_images = [np.transpose(sv, (0, 2, 3, 1)) for sv in shap_values]\n","  shap_sum = np.sum(np.abs(shap_values), axis=(1,2,3))\n","\n","  return shap_values_images, shap_values, shap_sum\n","\n","\n","def get_one_image_per_class(dataset):\n","    \"\"\"Pobiera po jednym obrazie z każdej klasy.\"\"\"\n","    images_per_class = {}\n","    for image, label in dataset:\n","        if label not in images_per_class:\n","            images_per_class[label] = image\n","        if len(images_per_class) == 10:\n","            break\n","    return list(images_per_class.values())\n","\n","class_indices = {}\n","subset_indices = []\n","for idx, (img, label) in enumerate(test_set):\n","    if label not in class_indices:\n","        class_indices[label] = idx\n","        subset_indices.append(idx)\n","    if len(subset_indices) == 10:\n","        break\n","\n","subset = Subset(test_set, subset_indices)\n","data_loader = DataLoader(subset, batch_size=10, shuffle=False)\n","\n","import torch\n","\n","def extract_data(dataset):\n","    images = []\n","    labels = []\n","    for img, label in dataset:\n","        images.append(img.cpu())\n","        labels.append(label)\n","\n","    images = torch.stack(images).cpu()\n","    labels = torch.tensor(labels).cpu()\n","    return images, labels\n","\n","def get_balanced_subset(dataset, samples_per_class=100):\n","    class_indices = defaultdict(list)\n","\n","    for idx, (img, label) in enumerate(dataset):\n","        class_indices[label].append(idx)\n","\n","    selected_indices = []\n","    for label, indices in class_indices.items():\n","        if len(indices) < samples_per_class:\n","            raise ValueError(f\"Klasa {label} ma tylko {len(indices)} próbek.\")\n","        selected_indices.extend(random.sample(indices, samples_per_class))\n","\n","    return Subset(dataset, selected_indices)\n","\n","balanced_subset = get_balanced_subset(test_set, samples_per_class=100)\n","\n","test_images, test_labels = extract_data(balanced_subset)\n","\n","print(len(test_images))\n","\n","\n","fau_shap_values_images, fau_shap_values, fau_shap_sum = calculate_shap(fau_model, test_images)\n","koo_shap_values_images, koo_shap_values, koo_shap_sum = calculate_shap(koo_model, test_images)\n","sei_shap_values_images, sei_shap_values, sei_shap_sum = calculate_shap(sei_model, test_images)\n","seb_shap_values_images, seb_shap_values, seb_shap_sum = calculate_shap(seb_model, test_images)\n","amn_shap_values_images, amn_shap_values, amn_shap_sum = calculate_shap(amn_model, test_images)\n","rt_shap_values_images, rt_shap_values, rt_shap_sum = calculate_shap(rt_model, test_images)"]},{"cell_type":"markdown","source":["# Count ϕ"],"metadata":{"id":"Tk8M_o1nwJsH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"I2siKGMSRvP8"},"outputs":[],"source":["tmp_fau = fau_shap_sum[np.arange(len(test_labels)), test_labels]\n","tmp_koo = koo_shap_sum[np.arange(len(test_labels)), test_labels]\n","tmp_sei = sei_shap_sum[np.arange(len(test_labels)), test_labels]\n","tmp_seb = seb_shap_sum[np.arange(len(test_labels)), test_labels]\n","tmp_amn = amn_shap_sum[np.arange(len(test_labels)), test_labels]\n","tmp_rt = rt_shap_sum[np.arange(len(test_labels)), test_labels]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"reB-Q4qfh6TL"},"outputs":[],"source":["print(np.sum(tmp_fau / np.sum(fau_shap_sum, axis = 1)) / np.sum(tmp_rt / np.sum(rt_shap_sum, axis = 1)))\n","print(np.sum(tmp_koo / np.sum(koo_shap_sum, axis = 1)) / np.sum(tmp_rt / np.sum(rt_shap_sum, axis = 1)))\n","print(np.sum(tmp_sei / np.sum(sei_shap_sum, axis = 1)) / np.sum(tmp_rt / np.sum(rt_shap_sum, axis = 1)))\n","print(np.sum(tmp_seb / np.sum(seb_shap_sum, axis = 1)) / np.sum(tmp_rt / np.sum(rt_shap_sum, axis = 1)))\n","print(np.sum(tmp_amn / np.sum(amn_shap_sum, axis = 1)) / np.sum(tmp_rt / np.sum(rt_shap_sum, axis = 1)))"]},{"cell_type":"markdown","source":["# Count accuracy"],"metadata":{"id":"femrldp32vEW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ogj3EzRoifjU"},"outputs":[],"source":["balanced_loader = torch.utils.data.DataLoader(\n","    balanced_subset, batch_size=128, shuffle=True, num_workers=2\n",")\n","\n","print(accuracy(fau_model, balanced_loader))\n","print(accuracy(koo_model, balanced_loader))\n","print(accuracy(sei_model, balanced_loader))\n","print(accuracy(seb_model, balanced_loader))\n","print(accuracy(amn_model, balanced_loader))\n","print(accuracy(rt_model, balanced_loader))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"drnRvv-tkgCb"},"outputs":[],"source":["print(accuracy(fau_model, balanced_loader) / accuracy(rt_model, balanced_loader))\n","print(accuracy(koo_model, balanced_loader) / accuracy(rt_model, balanced_loader))\n","print(accuracy(sei_model, balanced_loader) / accuracy(rt_model, balanced_loader))\n","print(accuracy(seb_model, balanced_loader) / accuracy(rt_model, balanced_loader))\n","print(accuracy(amn_model, balanced_loader) / accuracy(rt_model, balanced_loader))"]},{"cell_type":"code","source":["print((np.sum((fau_base_values)) - np.sum((rt_base_values))) /10)\n","print((np.sum((koo_base_values)) - np.sum((rt_base_values))) /10)\n","print((np.sum((sei_base_values)) - np.sum((rt_base_values))) /10)\n","print((np.sum((seb_base_values)) - np.sum((rt_base_values))) /10)\n","print((np.sum((amn_base_values)) - np.sum((rt_base_values))) /10)"],"metadata":{"id":"x8h0x2Nn6FZ6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Compare plot on 10 random images"],"metadata":{"id":"jn7WUQpW4iRR"}},{"cell_type":"code","source":["import shap\n","\n","def calculate_shap(model, data_loader):\n","  images, _ = next(iter(data_loader))\n","  images = images.to(DEVICE)\n","\n","  explainer = shap.GradientExplainer(model, images)\n","\n","  shap_values = explainer.shap_values(images)\n","\n","  shap_values_images = [np.transpose(sv, (0, 2, 3, 1)) for sv in shap_values]\n","  shap_sum = np.sum(np.abs(shap_values), axis=(1,2,3))\n","\n","  return shap_values_images, shap_values, shap_sum\n","\n","\n","def get_one_image_per_class(dataset):\n","    images_per_class = {}\n","    for image, label in dataset:\n","        if label not in images_per_class:\n","            images_per_class[label] = image\n","        if len(images_per_class) == 10:\n","            break\n","    return list(images_per_class.values())\n","\n","class_indices = {}\n","subset_indices = []\n","\n","for idx, (img, label) in enumerate(test_set):\n","    if label not in class_indices:\n","        class_indices[label] = idx\n","        subset_indices.append((label, idx))\n","    if len(class_indices) == 10:\n","        break\n","\n","subset_indices.sort(key=lambda x: x[0])\n","\n","sorted_indices = [idx for _, idx in subset_indices]\n","\n","subset = Subset(test_set, sorted_indices)\n","data_loader = DataLoader(subset, batch_size=10, shuffle=False)\n","\n","\n","fau_shap_values_images, fau_shap_values, fau_shap_sum = calculate_shap(fau_model, data_loader)\n","koo_shap_values_images, koo_shap_values, koo_shap_sum = calculate_shap(koo_model, data_loader)\n","sei_shap_values_images, sei_shap_values, sei_shap_sum = calculate_shap(sei_model, data_loader)\n","seb_shap_values_images, seb_shap_values, seb_shap_sum = calculate_shap(seb_model, data_loader)\n","amn_shap_values_images, amn_shap_values, amn_shap_sum = calculate_shap(amn_model, data_loader)\n","rt_shap_values_images, rt_shap_values, rt_shap_sum = calculate_shap(rt_model, data_loader)\n","org_shap_values_images, org_shap_values, org_shap_sum = calculate_shap(model, data_loader)"],"metadata":{"id":"qu_w5Dh38kXE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import display, HTML\n","\n","images, _ = next(iter(data_loader))\n","images = images.to(DEVICE)\n","images_np = images.permute(0, 2, 3, 1).cpu().numpy()\n","\n","def denormalize(img):\n","    img = img * 0.5 + 0.5\n","    img = np.clip(img, 0, 1)\n","    return img\n","\n","images_np = denormalize(images_np)\n","print(np.array(fau_shap_values_images).shape)\n","\n","fau_shap_values_images_fix = list(np.transpose(fau_shap_values, (4, 0, 2, 3, 1)))\n","koo_shap_values_images_fix = list(np.transpose(koo_shap_values, (4, 0, 2, 3, 1)))\n","sei_shap_values_images_fix = list(np.transpose(sei_shap_values, (4, 0, 2, 3, 1)))\n","seb_shap_values_images_fix = list(np.transpose(seb_shap_values, (4, 0, 2, 3, 1)))\n","amn_shap_values_images_fix = list(np.transpose(amn_shap_values, (4, 0, 2, 3, 1)))\n","rt_shap_values_images_fix = list(np.transpose(rt_shap_values, (4, 0, 2, 3, 1)))\n","org_shap_values_images_fix = list(np.transpose(org_shap_values, (4, 0, 2, 3, 1)))"],"metadata":{"id":"igtaWfz68v0n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import shap\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","num_samples = len(images_np)\n","\n","fau_selected_shap_values = [fau_shap_values_images_fix[i][i] for i in range(num_samples)]\n","koo_selected_shap_values = [koo_shap_values_images_fix[i][i] for i in range(num_samples)]\n","sei_selected_shap_values = [sei_shap_values_images_fix[i][i] for i in range(num_samples)]\n","seb_selected_shap_values = [seb_shap_values_images_fix[i][i] for i in range(num_samples)]\n","amn_selected_shap_values = [amn_shap_values_images_fix[i][i] for i in range(num_samples)]\n","rt_selected_shap_values = [rt_shap_values_images_fix[i][i] for i in range(num_samples)]\n","org_selected_shap_values = [org_shap_values_images_fix[i][i] for i in range(num_samples)]\n","\n","def ensure_channel_dim(shap_array):\n","    if shap_array.ndim == 3:\n","        return np.expand_dims(shap_array, axis=-1)\n","    return shap_array\n","\n","fau_selected_shap_values = ensure_channel_dim(np.array(fau_selected_shap_values))\n","koo_selected_shap_values = ensure_channel_dim(np.array(koo_selected_shap_values))\n","sei_selected_shap_values = ensure_channel_dim(np.array(sei_selected_shap_values))\n","seb_selected_shap_values = ensure_channel_dim(np.array(seb_selected_shap_values))\n","amn_selected_shap_values = ensure_channel_dim(np.array(amn_selected_shap_values))\n","rt_selected_shap_values = ensure_channel_dim(np.array(rt_selected_shap_values))\n","org_selected_shap_values = ensure_channel_dim(np.array(org_selected_shap_values))\n","\n","selected_shap_values = [\n","    org_selected_shap_values,\n","    rt_selected_shap_values,\n","    fau_selected_shap_values,\n","    koo_selected_shap_values,\n","    sei_selected_shap_values,\n","    seb_selected_shap_values,\n","    amn_selected_shap_values,\n","]\n","\n","print(\"Images shape:\", images_np.shape)\n","for idx, shap_val in enumerate(selected_shap_values):\n","    print(f\"SHAP values shape [{idx}]:\", shap_val.shape)\n","\n","label = [\"Original\", \"Retrained\", \"Fauchan\", \"Kookmin\", \"Seif\", \"Sebastian\", \"Amnesiacs\"]\n","\n","shap.image_plot(selected_shap_values, images_np)\n","plt.show()"],"metadata":{"id":"3EIWOCy_88ku"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["kXEWkDTTn4iO","WsOIZhKChv2G","6lukFE71wRa4","XyY_ieK1yo7q","4SkJ-KYR5G1O"],"gpuType":"T4","provenance":[{"file_id":"1m1V4k_OITalqvvnbMkkN055avUSuChht","timestamp":1740390179821}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":0}